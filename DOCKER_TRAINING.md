Docker Training (One‑click)

Prerequisites
- Linux host with NVIDIA GPU(s) and drivers installed.
- Docker Engine 24+ and Docker Compose plugin.
- NVIDIA Container Toolkit installed (nvidia-container-toolkit) so Docker can see GPUs.

Layout assumptions
- Base model in `basemodel/` (mounted to `/tmp/workspace/source_model`).
- Training data CSVs: by default we point `TRAIN_FILE_DIR` to the repo root inside the container, so a file like `translate_0.csv` in repo root will be picked up automatically. You can also add more CSVs to the repo root.
- Training outputs saved to `./output/` on host (mounted from `/tmp/workspace/target_model`).

Quick start
- One command: `bash scripts/train_docker.sh`
  - Builds the training image and runs the `trainer` service with GPU support.
  - Inside the container, we copy `train-0/prepare_v2.py` to `/tmp/workspace/train/` to match paths expected by `train-0/train.sh`.

What runs under the hood
- Base image: `hiyouga/llamafactory:latest` (includes `llamafactory-cli`).
- Command executed (inside container):
  - `cp -f /tmp/workspace/train/train-0/prepare_v2.py /tmp/workspace/train/`
  - `cd /tmp/workspace/train/train-0 && bash train.sh`
  - `train.sh` prepares `train.yaml` and launches `llamafactory-cli train train.yaml` twice (two stages when `--do_pt`).

Volumes and paths
- `./` → `/tmp/workspace/train` (repo code; includes CSVs in repo root)
- `./basemodel` → `/tmp/workspace/source_model` (read-only)
- `./output` → `/tmp/workspace/target_model` (training artifacts)

Notes
- If your CSVs live elsewhere, you can set `TRAIN_FILE_DIR` via `docker-compose.yml` or pass at runtime: `docker compose run -e TRAIN_FILE_DIR=/some/dir --gpus all trainer`.
- If your GPU doesn’t support BF16, the default config generated by `prepare_v2.py` may need changes to use FP16. This repo’s code is left unchanged per request; adjust later if needed.
- For repeated runs, clear or version the `./output` directory to avoid overwriting.

